# Linear Regression Optimization Study

This project investigates the convergence behavior of different optimization methods in Linear Regression.

The study compares:

- Normal Equation (closed-form solution)
- Batch Gradient Descent (BGD)
- Stochastic Gradient Descent (SGD)
- Mini-Batch Gradient Descent (MBGD)

The full mathematical derivation of the cost function, gradient formulation, matrix calculus transition, experimental setup, and result analysis are documented inside the Google Colab notebook.

The project focuses on:
- Manual gradient derivation
- Matrix-based implementation
- Convergence comparison
- Numerical consistency across methods

## How to Run

Open the notebook in Google Colab and execute all cells sequentially.

## Requirements

numpy  
matplotlib
